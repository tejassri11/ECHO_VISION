

---

# **Vision Assistant For Blind Person**

An advanced **AI-powered vision assistant** that can **describe scenes**, **read text**, and **detect currency** from a live camera feed.
The project integrates **Computer Vision**, **Deep Learning**, and **Real-Time Processing** using:

* **BLIP** â†’ Image captioning
* **MiDaS** â†’ Depth estimation
* **PaddleOCR** â†’ Text detection & extraction
* **Custom Currency Detector** â†’ Indian currency classification
* **Flask** â†’ Backend API
* **Web UI (HTML/JS)** â†’ Live camera streaming + interactive controls
* **OpenCV** â†’ Frame handling & processing
* **pyttsx3** â†’ Text-to-Speech output

---

## ğŸš€ **Features**

### ğŸ”¹ **1. Scene Description (BLIP Model)**

* Generates human-like captions for any object or environment in front of the camera.
* Powered by **BLIP (Bootstrapping Language-Image Pretraining)**.
* Gives highly accurate and contextual descriptions.

### ğŸ”¹ **2. Depth Estimation (MiDaS)**

* Predicts relative depth for everything in the scene.
* Produces a heatmap-style depth view.

### ğŸ”¹ **3. OCR â€“ Text Reading (PaddleOCR)**

* Detects and reads text from real-world objects (books, labels, signs, screens).
* Supports:

  * Multi-line text
  * Confidence scoring
  * Angle correction

### ğŸ”¹ **4. Currency Detection**

* Identifies Indian currency notes.
* Gives denomination & confidence score.
* Useful for visually impaired or general automation.

### ğŸ”¹ **5. Real-Time Processing via Flask**

* Live video feed from the browser.
* Captured frames are sent to Flask API for inference.
* Response displayed instantly on UI.

### ğŸ”¹ **6. Text-to-Speech Output**

* Every result (caption / text / currency) is spoken aloud.
* Uses **pyttsx3** for offline TTS.

---

## ğŸ§  **Tech Stack**

### **Backend**

* Python
* Flask
* BLIP (Transformers)
* MiDaS (Torch Hub)
* PaddleOCR
* OpenCV
* TensorFlow/Keras (Currency Model)
* pyttsx3

### **Frontend**

* HTML
* CSS
* JavaScript
* Web Speech API
* getUserMedia (Webcam Access)

---

## ğŸ—‚ï¸ **Project Structure**

```
vision-assistant/
â”‚
â”œâ”€â”€ server.py                # Main Flask server
â”œâ”€â”€ scene_describer.py       # BLIP caption + MiDaS depth
â”œâ”€â”€ text_reader.py           # PaddleOCR-based text detection
â”œâ”€â”€ currency_detector.py     # Currency classification module
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ script.js            # Frontend JS
â”‚   â”œâ”€â”€ style.css            # Styling
â”‚   â””â”€â”€ index.html           # Main UI
â””â”€â”€ models/
    â””â”€â”€ currency_model.h5    # Custom-trained currency model
```

---

## âš™ï¸ **Installation**

### **1ï¸âƒ£ Create Virtual Environment**

```bash
python -m venv vision_env
```

### **2ï¸âƒ£ Activate Environment**

Windows:

```bash
vision_env\Scripts\activate
```

### **3ï¸âƒ£ Install Dependencies**

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ **Run the Application**

Start Flask server:

```bash
python server.py
```

Open in browser:

```
http://127.0.0.1:5000
```

---

## ğŸ§ª **How It Works**

### 1ï¸âƒ£ User opens web app â†’ camera feed starts

### 2ï¸âƒ£ User selects:

* **Describe Scene**
* **Read Text**
* **Detect Currency**

### 3ï¸âƒ£ Frame sent to Flask backend

### 4ï¸âƒ£ Backend runs:

* BLIP â†’ Caption
* MiDaS â†’ Depth
* PaddleOCR â†’ Text
* Custom Model â†’ Currency

### 5ï¸âƒ£ Result returned + spoken aloud

## Preview
<img width="1873" height="954" alt="Screenshot 2025-11-21 002833" src="https://github.com/user-attachments/assets/62439d9b-677d-47ce-9975-a7bc67f43e47" />
<img width="1616" height="961" alt="Screenshot 2025-11-21 002138" src="https://github.com/user-attachments/assets/d9a97980-d7eb-4f15-bf9b-20720b1f782e" />
<img width="1882" height="904" alt="Screenshot 2025-11-21 003527" src="https://github.com/user-attachments/assets/4c0cb05f-965f-4b26-b75c-158eeadfea7f" />



---
